{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ab94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.general import non_max_suppression\n",
    "from augmentations import mix_augmentaion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363d93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 'runs/train/exp2/weights/best_288.pt'\n",
    "data = 'data/plate.yaml'\n",
    "input_size = 512\n",
    "conf_thres = 0.5\n",
    "iou_thres = 0.5\n",
    "device = 'cuda:2'\n",
    "aug_licence = mix_augmentaion()\n",
    "\n",
    "with open(data) as f:\n",
    "    data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "classes = data['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c631f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs/train/exp2/weights/best_288.pt, epoch 288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): Sequential(\n",
       "    (0): Conv(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (1): Conv(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (2): Conv(\n",
       "      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (3): Conv(\n",
       "      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (4): Conv(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (5): Conv(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (6): Concat()\n",
       "    (7): Conv(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (8): MP(\n",
       "      (m): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (9): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (10): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (11): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (12): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (13): Concat()\n",
       "    (14): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (15): MP(\n",
       "      (m): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (16): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (17): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (18): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (19): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (20): Concat()\n",
       "    (21): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (22): MP(\n",
       "      (m): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (23): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (24): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (25): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (26): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (27): Concat()\n",
       "    (28): Conv(\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (29): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (30): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (31): SP(\n",
       "      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (32): SP(\n",
       "      (m): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (33): SP(\n",
       "      (m): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (34): Concat()\n",
       "    (35): Conv(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (36): Concat()\n",
       "    (37): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (38): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (39): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (40): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (41): Concat()\n",
       "    (42): Conv(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (43): Conv(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (44): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (45): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (46): Concat()\n",
       "    (47): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (48): Conv(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (49): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (50): Conv(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (51): Concat()\n",
       "    (52): Conv(\n",
       "      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (53): Conv(\n",
       "      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (54): Conv(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (55): Conv(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (56): Concat()\n",
       "    (57): Conv(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (58): Conv(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (59): Concat()\n",
       "    (60): Conv(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (61): Conv(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (62): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (63): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (64): Concat()\n",
       "    (65): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (66): Conv(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (67): Concat()\n",
       "    (68): Conv(\n",
       "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (69): Conv(\n",
       "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (70): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (71): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (72): Concat()\n",
       "    (73): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (74): Conv(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (75): Conv(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (76): Conv(\n",
       "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (77): IDetect(\n",
       "      (m): ModuleList(\n",
       "        (0): Conv2d(128, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(256, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(512, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (ia): ModuleList(\n",
       "        (0): ImplicitA()\n",
       "        (1): ImplicitA()\n",
       "        (2): ImplicitA()\n",
       "      )\n",
       "      (im): ModuleList(\n",
       "        (0): ImplicitM()\n",
       "        (1): ImplicitM()\n",
       "        (2): ImplicitM()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(weight)\n",
    "print('Loaded {}, epoch {}'.format(weight, checkpoint['epoch']))\n",
    "model = checkpoint['model'].cuda().to(device)#.half()\n",
    "\n",
    "model.eval()\n",
    "#torch.save(model.state_dict(),'runs/train/exp2/weights/best_288_state_dict.pt')\n",
    "#print(model.forward_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2855dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_with_diffusion(\n",
       "  (model): Sequential(\n",
       "    (0): Conv(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (1): Conv(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (2): Conv(\n",
       "      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (3): Conv(\n",
       "      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (4): Conv(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (5): Conv(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (6): Concat()\n",
       "    (7): Conv(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (8): MP(\n",
       "      (m): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (9): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (10): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (11): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (12): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (13): Concat()\n",
       "    (14): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (15): MP(\n",
       "      (m): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (16): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (17): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (18): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (19): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (20): Concat()\n",
       "    (21): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (22): MP(\n",
       "      (m): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (23): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (24): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (25): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (26): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (27): Concat()\n",
       "    (28): Conv(\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (29): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (30): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (31): SP(\n",
       "      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (32): SP(\n",
       "      (m): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (33): SP(\n",
       "      (m): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (34): Concat()\n",
       "    (35): Conv(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (36): Concat()\n",
       "    (37): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (38): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (39): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (40): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (41): Concat()\n",
       "    (42): Conv(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (43): Conv(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (44): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (45): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (46): Concat()\n",
       "    (47): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (48): Conv(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (49): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (50): Conv(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (51): Concat()\n",
       "    (52): Conv(\n",
       "      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (53): Conv(\n",
       "      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (54): Conv(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (55): Conv(\n",
       "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (56): Concat()\n",
       "    (57): Conv(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (58): Conv(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (59): Concat()\n",
       "    (60): Conv(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (61): Conv(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (62): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (63): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (64): Concat()\n",
       "    (65): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (66): Conv(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (67): Concat()\n",
       "    (68): Conv(\n",
       "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (69): Conv(\n",
       "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (70): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (71): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (72): Concat()\n",
       "    (73): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (74): Conv(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (75): Conv(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (76): Conv(\n",
       "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (77): IDetect(\n",
       "      (m): ModuleList(\n",
       "        (0): Conv2d(128, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(256, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(512, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (ia): ModuleList(\n",
       "        (0): ImplicitA()\n",
       "        (1): ImplicitA()\n",
       "        (2): ImplicitA()\n",
       "      )\n",
       "      (im): ModuleList(\n",
       "        (0): ImplicitM()\n",
       "        (1): ImplicitM()\n",
       "        (2): ImplicitM()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.yolo_with_diffusion import Model_with_diffusion\n",
    "cfgyolotiny='/data/licence_plate/_yolo/yolov7/cfg/training/yolov7-tiny.yaml'\n",
    "hypyolotiny='/data/licence_plate/_yolo/yolov7/cfg/deploy/hyp.scratch.tiny.yaml'\n",
    "best_288_state_dict='runs/train/exp2/weights/best_288_state_dict.pt'\n",
    "nc=35\n",
    "with open(hypyolotiny) as f:\n",
    "    hyp = yaml.load(f, Loader=yaml.SafeLoader)  # load hyps\n",
    "\n",
    "model = Model_with_diffusion(cfgyolotiny, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)#.half()  # create\n",
    "model.load_state_dict(torch.load(best_288_state_dict))\n",
    "model.eval()\n",
    "#print(model.m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc5f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Time embed used ?  True\n",
      "AOLP\n",
      "Loading :  /data/licence_plate/Cold-Diffusion-Models/generator_model_512_fp16_217k.pt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/data/licence_plate/Cold-Diffusion-Models/demixing-diffusion-pytorch/')\n",
    "\n",
    "from demixing_diffusion_pytorch.demixing_diffusion_licencepairs_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "\n",
    "model_path='/data/licence_plate/Cold-Diffusion-Models/generator_model_512_fp16_217k.pt'\n",
    "data_path='/data/licence_plate/_plate/generated_data/result2/img/'\n",
    "\n",
    "\n",
    "diffusionmodel = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    channels=3,\n",
    "    with_time_emb=not(False),\n",
    "    residual=False\n",
    ").cuda().to(device)#.half()\n",
    "\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    diffusionmodel,\n",
    "    image_size = 512,\n",
    "    channels = 3,\n",
    "    timesteps = 200,        # number of steps\n",
    "    loss_type = 'l1',                   # L1 or L2\n",
    "    train_routine = 'Final',\n",
    "    sampling_routine = 'x0_step_down',\n",
    ").cuda().to(device)#.half()\n",
    "\n",
    "diffusion = torch.nn.DataParallel(diffusion, device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "diffusion_trainer = Trainer(\n",
    "    diffusion,\n",
    "    [data_path],\n",
    "    image_size = 512,\n",
    "    train_batch_size = 8,\n",
    "    train_lr = 2e-5,\n",
    "    train_num_steps = 700000, # total training steps\n",
    "    gradient_accumulate_every = 2,      # gradient accumulation steps\n",
    "    ema_decay = 0.995,                  # exponential moving average decay\n",
    "    fp16 = True,                       # turn on mixed precision training with apex\n",
    "    results_folder = './',\n",
    "    load_path = model_path,\n",
    "    dataset = 'AOLP'\n",
    ")\n",
    "\n",
    "#self.transform = transforms.Compose([\n",
    "#    transforms.Resize((int(image_size*1.12), int(image_size*1.12))),\n",
    "#    transforms.CenterCrop(image_size),\n",
    "#    transforms.ToTensor(),\n",
    "#    transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "#])\n",
    "\n",
    "#def __len__(self):\n",
    "#    return len(self.paths)\n",
    "\n",
    "#def __getitem__(self, index):\n",
    "#    path = self.paths[index]\n",
    "#    path = os.path.join(path)\n",
    "    #print(path+'\\n')\n",
    "#    index_img= path.find('/img/')\n",
    "#    path_blur = path[:index_img]+'/img_blur/'+path[index_img+5:]\n",
    "\n",
    "#    img = Image.open(path)\n",
    "#    img = img.convert('RGB')\n",
    "#    img_blur = Image.open(path_blur)\n",
    "#    img_blur = img_blur.convert('RGB')\n",
    "#    return self.transform(img),self.transform(img_blur)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_,og_img = next(self.dl)\n",
    "#og_img = og_img.cuda()\n",
    "\n",
    "\n",
    "#X_0s, X_ts = self.ema_model.module.all_sample(batch_size=batches, img=og_img, times=s_times)\n",
    "\n",
    "#og_img = (og_img + 1) * 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48fccc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img, size):\n",
    "    h, w, c = img.shape\n",
    "    if not (h == size and w == size):\n",
    "        img = img.copy()\n",
    "        scale_x = float(size / w)\n",
    "        scale_y = float(size / h)\n",
    "        ratio = min(scale_x, scale_y)\n",
    "        nw, nh = int(w*ratio), int(h*ratio)\n",
    "        new_img = cv2.resize(img, (nw, nh))\n",
    "\n",
    "        blank = np.zeros((size, size, c))\n",
    "        dw, dh = (size-nw)//2, (size-nh)//2\n",
    "        blank[dh: dh+nh, dw: dw+nw] = new_img\n",
    "        meta = {'nw': nw, 'nh': nh, 'dw': dw, 'dh': dh, 'w': w, 'h': h}\n",
    "        return blank, meta\n",
    "    else:\n",
    "        meta = {}\n",
    "        return img, meta\n",
    "\n",
    "def warp_affine(pt, M):\n",
    "    new_pt = np.array([pt[0], pt[1], 1.], dtype=np.float32).T\n",
    "    new_pt = np.dot(M, new_pt)\n",
    "    return new_pt\n",
    "\n",
    "def affine_transform(dets, meta):\n",
    "    '''\n",
    "    Transfer input-sized perdictions to original-sized coordinate. (Anchor)\n",
    "    Input:\n",
    "        dets = [1(batch), num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "        meta = {'nw': resize_w, \n",
    "                'nh': resize_h, \n",
    "                'dw': offset_w, \n",
    "                'dh': offset_h, \n",
    "                'w': original_size_w, \n",
    "                'h': original_size_h}\n",
    "    '''\n",
    "    dets = np.array([x.cpu().numpy() for x in dets[0]])\n",
    "    if len(meta)>0:\n",
    "        p1 = np.float32([[0, 0], [0, meta['nh']], [meta['nw'], 0]])\n",
    "        p2 = np.float32([[0, 0], [0, meta['h']], [meta['w'], 0]])\n",
    "        M = cv2.getAffineTransform(p1, p2)\n",
    "\n",
    "        for i in range(dets.shape[0]):\n",
    "            dets[i, 0] -= meta['dw']\n",
    "            dets[i, 1] -= meta['dh']\n",
    "            dets[i, 2] -= meta['dw']\n",
    "            dets[i, 3] -= meta['dh']\n",
    "\n",
    "            dets[i, 0:2] = warp_affine(dets[i, 0:2], M)\n",
    "            dets[i, 2:4] = warp_affine(dets[i, 2:4], M)\n",
    "        return dets\n",
    "    else:\n",
    "        return dets\n",
    "\n",
    "def preproccess_img(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img, meta = resize(img, input_size)\n",
    "    img = (img / 255.).astype(np.float32)\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x512x512\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "    #img = img.half()\n",
    "    return img, meta\n",
    "\n",
    "def run(img_path):\n",
    "    '''\n",
    "    Return: output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "    '''\n",
    "    img, meta = preproccess_img(img_path)\n",
    "    with torch.no_grad():\n",
    "        ret = model(img)\n",
    "        ret = non_max_suppression(ret[0], conf_thres=conf_thres, iou_thres=iou_thres, labels=[], multi_label=True)\n",
    "    return ret, meta\n",
    "\n",
    "#X_0s, X_ts = self.ema_model.module.all_sample(batch_size=batches, img=og_img, times=s_times)\n",
    "#step = torch.full((batch_size,), t - 1, dtype=torch.long).cuda()\n",
    "#x1_bar = self.denoise_fn(img, step)\n",
    "\n",
    "def run_withdiffusion(img_path,times=200):\n",
    "    '''\n",
    "    Return: output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "    '''\n",
    "    img, meta = preproccess_img(img_path)\n",
    "    with torch.no_grad():\n",
    "        #ret_bf,y = model.forward_submodel(img,model.before_diffusion_model,output_y=True)\n",
    "        img = (img*2)-1\n",
    "        batch_size = img.shape[0]\n",
    "        step = torch.full((batch_size,), times - 1, dtype=torch.long).cuda().to(device)#.half()\n",
    "        img_diffusion = diffusion_trainer.ema_model.module.denoise_fn(img,step)\n",
    "        img_diffusion = (img_diffusion + 1) * 0.5\n",
    "\n",
    "        ret = model(img_diffusion)\n",
    "        ret = non_max_suppression(ret[0], conf_thres=conf_thres, iou_thres=iou_thres, labels=[], multi_label=True)\n",
    "    return ret, meta\n",
    "\n",
    "def run_aug_withdiffusion(img_path,times=200,ratio=1.0):\n",
    "    '''\n",
    "    Return: output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "    '''\n",
    "    img, meta = preproccess_img(img_path)\n",
    "    img = aug_licence.batch_data_add_licence_aug(img, ratio)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #ret_bf,y = model.forward_submodel(img,model.before_diffusion_model,output_y=True)\n",
    "        img = (img*2)-1\n",
    "        batch_size = img.shape[0]\n",
    "        step = torch.full((batch_size,), times - 1, dtype=torch.long).cuda().to(device)#.half()\n",
    "        img_diffusion = diffusion_trainer.ema_model.module.denoise_fn(img,step)\n",
    "        img_diffusion = (img_diffusion + 1) * 0.5\n",
    "\n",
    "        ret = model(img_diffusion)\n",
    "        ret = non_max_suppression(ret[0], conf_thres=conf_thres, iou_thres=iou_thres, labels=[], multi_label=True)\n",
    "    return ret, meta\n",
    "\n",
    "\n",
    "\n",
    "def run_aug(img_path,ratio=1.0):\n",
    "    '''\n",
    "    Return: output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "    '''\n",
    "    img, meta = preproccess_img(img_path)\n",
    "    b, c, h, w = img.shape\n",
    "    aug_licence.imshape=[h,w]\n",
    "    aug_licence.random_parameter()\n",
    "\n",
    "    img = aug_licence.batch_data_add_licence_aug(img, ratio)\n",
    "    with torch.no_grad():\n",
    "        ret = model(img)\n",
    "        ret = non_max_suppression(ret[0], conf_thres=conf_thres, iou_thres=iou_thres, labels=[], multi_label=True)\n",
    "    return ret, meta\n",
    "\n",
    "#==================================================================\n",
    "\n",
    "def transfer_label(lab):\n",
    "    '''\n",
    "    Transfer label(string) into list(int).\n",
    "    Input:\n",
    "        lab: string list ['x_min,y_min,x_max,y_max,plate']\n",
    "            i.e. ['14,71,83,105,FS799', '215,188,324,240,DP4846']\n",
    "    Return:\n",
    "        new_lab: int list [x_min, y_min, x_max, y_max]\n",
    "            i.e. [[14,71,83,105], [215,188,324,240]]\n",
    "    '''\n",
    "    new_lab = []\n",
    "    for l in lab:\n",
    "        _l = l.split(',')\n",
    "        x1 = int(_l[0])\n",
    "        y1 = int(_l[1])\n",
    "        x2 = int(_l[2])\n",
    "        y2 = int(_l[3])\n",
    "        new_lab.append([x1, y1, x2, y2])\n",
    "    \n",
    "    return new_lab\n",
    "\n",
    "def get_iou(bb1, bb2):\n",
    "    '''\n",
    "    Input:\n",
    "        bb1(groundtruth) = [left_top_x, y, right_bottom_x, y]\n",
    "        bb2(predict_point) = [left_top_x, y, right_bottom_x, y]\n",
    "    '''\n",
    "    x_left = max(bb1[0], bb2[0])\n",
    "    y_top = max(bb1[1], bb2[1])\n",
    "    x_right = min(bb1[2], bb2[2])\n",
    "    y_bottom = min(bb1[3], bb2[3])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou\n",
    "\n",
    "def check_bbox(bb1, bb2):\n",
    "    '''\n",
    "    Check if bb2 is completely covered by bb1.\n",
    "    If so, return True, otherwise return False.\n",
    "    Input:\n",
    "        bb1(large region) = [x_min, y_min, x_max, y_max] (all int)\n",
    "        bb2(small region) = [x_min, y_min, x_max, y_max] (all int)\n",
    "    '''\n",
    "    # Assert bb1 is the larger one\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "    if bb1_area < bb2_area:\n",
    "        temp = bb2\n",
    "        bb2 = bb1\n",
    "        bb1 = temp\n",
    "    \n",
    "    f1 = bb1[0] <= bb2[0]\n",
    "    f2 = bb1[1] <= bb2[1]\n",
    "    f3 = bb1[2] >= bb2[2]\n",
    "    f4 = bb1[3] >= bb2[3]\n",
    "\n",
    "    if f1 and f2 and f3 and f4:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def group_plate(outp):\n",
    "    '''\n",
    "    Group each character into seperate plates.\n",
    "    Input:\n",
    "        outp = float[[x1, y1, x2, y2, score, class], ...]\n",
    "            i.e. [[     40.859      162.66      142.42      216.09      0.9668          34]\n",
    "                  [     125.08      178.28      138.83      208.91     0.93213           6]\n",
    "                  [         95      176.41      110.94      207.03      0.9248           8]\n",
    "                  [     110.16      177.34      125.16      208.28     0.91211           8]\n",
    "                  [     80.156      175.47      97.344      206.72     0.90674           4]\n",
    "                  [     45.039      173.44      64.414         205     0.89111          13]\n",
    "                  [     60.078      174.06      78.828      205.62     0.81445          17]]\n",
    "    Return: \n",
    "        a sorted list of dict [{'plate': int[x1, y1, x2, y2], 'char':[int[x1, y1, x2, y2, idx], ...]}, ...]\n",
    "             i.e. [{'plate': [40, 162, 142, 216], \n",
    "                    'char':  [[45, 173, 64, 205, 13],\n",
    "                             [60, 174, 78, 205, 17],\n",
    "                             [80, 175, 97, 206, 4],\n",
    "                             [95, 176, 110, 207, 8],\n",
    "                             [110, 177, 125, 208, 8],\n",
    "                             [125, 178, 138, 208, 6]]}]\n",
    "    '''\n",
    "    plates = []\n",
    "    chars = []\n",
    "    groups = []\n",
    "    for obj in outp:\n",
    "        if int(obj[-1]) == 34:\n",
    "            pla = [int(p) for p in obj[:4]]\n",
    "            group = {'plate': pla, 'char':[]}\n",
    "            groups.append(group)\n",
    "        else:\n",
    "            chars.append(obj)\n",
    "    for obj in chars:\n",
    "        cha = [int(obj[0]), int(obj[1]), int(obj[2]), int(obj[3]), int(obj[5])]\n",
    "        for g in groups:\n",
    "            pla = g['plate']\n",
    "            if check_bbox(pla, cha[:4]):\n",
    "                g['char'].append(cha)\n",
    "    ### Sort list\n",
    "    for g in groups:\n",
    "        g['char'] = sorted(g['char'], key=lambda x: x[0])\n",
    "    return groups\n",
    "\n",
    "def get_str(chars):\n",
    "    '''\n",
    "    Create plate string from indivisual detected character.\n",
    "    Input:\n",
    "        chars = a sorted list [int[x1, y1, x2, y2, idx], ...]]\n",
    "    Return:\n",
    "        plate_str = plate string, i.e. 'DH4886'\n",
    "    '''\n",
    "    s = ''\n",
    "    for o in chars:\n",
    "        c = o[-1]\n",
    "        s += classes[c]\n",
    "    return s\n",
    "\n",
    "def get_acc(inp, outp):\n",
    "    \"\"\"\n",
    "    Compute two strings character by character.\n",
    "    Input:\n",
    "        inp: ground truth(str)\n",
    "        outp: detected result(str)\n",
    "    Return:\n",
    "        m: number of groundtruth(int)\n",
    "        count: number of detect correctly(int)\n",
    "    \"\"\"\n",
    "    m = len(inp)\n",
    "    count = sum((Counter(inp) & Counter(outp)).values())\n",
    "    return m, count\n",
    "\n",
    "def compute_acc(outp, labels):\n",
    "    '''\n",
    "    Compute accuracy between detected results and labels.\n",
    "    Input:\n",
    "        outp = a list of dic [{'plate': int[x1, y1, x2, y2], 'char':[int[x1, y1, x2, y2, idx], ...]}, ...]\n",
    "        labels = a list of str, i.e. ['14,71,83,105,FS799', '215,188,324,240,DP4846']\n",
    "    Return:\n",
    "        total = the number of all characters in labels\n",
    "        correct = the number of correct-detected characters\n",
    "    '''\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for label in labels:\n",
    "        line = label.split(',')\n",
    "        plate_gt = [int(x) for x in line[:4]]\n",
    "        for g in outp:\n",
    "            if get_iou(plate_gt, g['plate']) >= 0.5:\n",
    "                detected_plate = get_str(g['char'])\n",
    "                t, c = get_acc(line[-1], detected_plate)\n",
    "                total += t\n",
    "                correct += c\n",
    "    return total, correct\n",
    "\n",
    "def labels_len(labels):\n",
    "    '''\n",
    "    Compute the number of characters in ground truth.\n",
    "    Input:\n",
    "        labels = a list of str, i.e. ['14,71,83,105,FS799', '215,188,324,240,DP4846']\n",
    "    Return:\n",
    "        num = the number of characters of all plates\n",
    "    '''\n",
    "    num = 0\n",
    "    for label in labels:\n",
    "        lines = label.split(',')\n",
    "        n = len(lines[-1])\n",
    "        num += n\n",
    "    return num\n",
    "\n",
    "def get_wer(r, h):\n",
    "    \"\"\"\n",
    "    Compute word_error_rate(WER) of two list of strings.\n",
    "    Input:\n",
    "        r = ground truth\n",
    "        h = predicted results\n",
    "    Return:\n",
    "        result = WER (presented in percentage)\n",
    "        sid = substitution + insertion + deletion\n",
    "        total = the number of groundtruth\n",
    "    \"\"\"\n",
    "    d = np.zeros((len(r) + 1) * (len(h) + 1), dtype=np.uint16)\n",
    "    d = d.reshape((len(r) + 1, len(h) + 1))\n",
    "    for i in range(len(r) + 1):\n",
    "        for j in range(len(h) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    for i in range(1, len(r) + 1):\n",
    "        for j in range(1, len(h) + 1):\n",
    "            if r[i - 1] == h[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    sid = d[len(r)][len(h)]\n",
    "    total = len(r)\n",
    "    result = float(sid) / total * 100\n",
    "\n",
    "    return result, sid, total\n",
    "\n",
    "def eval_dataset(results,label_data):\n",
    "    correct_plate = {}\n",
    "    total_p, correct_p, pred_p = 0, 0, 0\n",
    "    for k, v in label_data.items():\n",
    "        gt = transfer_label(v)\n",
    "        total_p += len(gt)\n",
    "        if k in results:\n",
    "            r = group_plate(results[k]) # r = [{'plate': int[x1, y1, x2, y2], 'char':[int[x1, y1, x2, y2, label], ...]}, ...]\n",
    "            pred_p += len(r)\n",
    "            correct_r = []\n",
    "            for _r in r:\n",
    "                for idx, bbox in enumerate(gt):\n",
    "                    if get_iou(bbox, _r['plate']) >= 0.5:\n",
    "                        n_r = {'plate': _r['plate'], 'char': _r['char'], 'idx': idx}\n",
    "                        correct_r.append(n_r)\n",
    "            num = len(correct_r)\n",
    "            if num > 0:\n",
    "                correct_plate[k] = correct_r\n",
    "                correct_p += num\n",
    "                \n",
    "    print(\"Number of Correctly Detected Plates =\", correct_p)\n",
    "    print(\"Number of Detected Plates =\", pred_p)\n",
    "    print(\"Number of All Plates =\", total_p)\n",
    "    print(\"Recall = {:.4f}\".format(correct_p/total_p))\n",
    "    print(\"Precision = {:.4f}\".format(correct_p/pred_p))\n",
    "\n",
    "    n_perfect = 0  ### number of perfectly recognized plates\n",
    "    n_sid = 0  ### number of failed recognized chars in detected\n",
    "    n_detected = 0  ### number of chars in detected plates\n",
    "\n",
    "    for k, v in label_data.items():\n",
    "        gt_strs = [s.split(',')[-1] for s in v]\n",
    "        if k in correct_plate:\n",
    "            objs = correct_plate[k]\n",
    "            for obj in objs: # obj = [{'plate': int[x1, y1, x2, y2], 'char':[int[x1, y1, x2, y2, label], ...], 'idx': int(i)}]\n",
    "                pred_str = [classes[x[-1]] for x in obj['char']]\n",
    "                wer, sid, t = get_wer(list(gt_strs[obj['idx']]), pred_str)\n",
    "                n_sid += sid\n",
    "                n_detected += t\n",
    "                if wer == 0:\n",
    "                    n_perfect += 1\n",
    "\n",
    "\n",
    "    print(\"Characters in Detected Plates = \", n_detected)\n",
    "    print(\"Error Characters (Detected) =\", n_sid)\n",
    "    print(\"World Error Rate (Detected) = {:.4f}\".format(n_sid/n_detected))\n",
    "\n",
    "    print(\"\\nNumber of Perfectly Recognized Plates = \", n_perfect)\n",
    "    print(\"Accuracy(Detected) = {:.4f}\".format(n_perfect/correct_p))\n",
    "    print(\"Accuracy(Groundtruth) = {:.4f}\".format(n_perfect/total_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ebff549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create label dictionary.\n",
    "Format: dic = {key: file_name(str), value: [obj1(str), obj2(str), ...]}\n",
    "        obj format = 'x_min, y_min, x_max, y_max, plate'\n",
    "   i.e. dic['train_LE_3'] = ['266,199,350,242,2972KK']\n",
    "        dic['train_LE_33'] = ['14,71,83,105,FS799', '215,188,324,240,DP4846']\n",
    "\"\"\"\n",
    "label_data = {}\n",
    "#label_txt = '/data/licence_plate/_plate/AOLP/label.txt'\n",
    "label_txt = '/data/licence_plate/_plate/weather/label.txt'\n",
    "\n",
    "#label_txt = 'E:/MTL_FTP/ChengJungC/dataset/AOLP/label.txt'\n",
    "#label_txt = 'E:/MTL_FTP/ChengJungC/dataset/weather/label.txt'\n",
    "label_file = open(label_txt, 'r')\n",
    "lines = label_file.readlines()\n",
    "for line in lines:\n",
    "    l = line.strip().split(' ')\n",
    "    name = l[0]\n",
    "    plates = l[1:]\n",
    "    label_data[name] = plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123ffc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_dir = '/data/licence_plate/_plate/AOLP/original/'\n",
    "img_dir = '/data/licence_plate/_plate/weather/original/'\n",
    "\n",
    "img_paths = os.listdir(img_dir)\n",
    "img_paths.sort()\n",
    "model.create_subnetwork()\n",
    "#model.before_diffusion_model.half()\n",
    "#model.after_diffusion_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3954fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test original network and after clip subnet should be all same(mean=max=min=0)\n",
    "# img_dir = 'E:/MTL_FTP/ChengJungC/dataset/AOLP/original/'\n",
    "#img_dir = 'E:/MTL_FTP/ChengJungC/dataset/weather/original/'\n",
    "\n",
    "results = {}\n",
    "for f in tqdm(img_paths, ncols=80):\n",
    "    if f.endswith('.jpg'):\n",
    "        bname = os.path.splitext(f)[0]\n",
    "        img_p = img_dir + f\n",
    "        ret1, ret2, meta = run_test_bfaf(img_p)\n",
    "        #print(ret1[1][0])\n",
    "        #print(ret2[1][0])\n",
    "        for idx_ret in range(len(ret1)):\n",
    "            print(str(idx_ret)+'_'+str(torch.mean(ret1[idx_ret][0]-ret2[idx_ret][0]))+str(torch.min(ret1[idx_ret][0]-ret2[idx_ret][0]))+str(torch.max(ret1[idx_ret][0]-ret2[idx_ret][0])))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32605ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2049/2049 [03:32<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Detected Plates = 2068\n",
      "Number of Detected Plates = 2528\n",
      "Number of All Plates = 2164\n",
      "Recall = 0.9556\n",
      "Precision = 0.8180\n",
      "Characters in Detected Plates =  12357\n",
      "Error Characters (Detected) = 1443\n",
      "World Error Rate (Detected) = 0.1168\n",
      "\n",
      "Number of Perfectly Recognized Plates =  1116\n",
      "Accuracy(Detected) = 0.5397\n",
      "Accuracy(Groundtruth) = 0.5157\n"
     ]
    }
   ],
   "source": [
    "# img_dir = 'E:/MTL_FTP/ChengJungC/dataset/AOLP/original/'\n",
    "#img_dir = 'E:/MTL_FTP/ChengJungC/dataset/weather/original/'\n",
    "\n",
    "results = {}\n",
    "for f in tqdm(img_paths, ncols=80):\n",
    "    if f.endswith('.jpg'):\n",
    "        bname = os.path.splitext(f)[0]\n",
    "        img_p = img_dir + f\n",
    "        ret, meta = run(img_p)\n",
    "        \n",
    "        if ret[0] is not None:\n",
    "            output = affine_transform(ret, meta) # output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "            results[bname] = output\n",
    "eval_dataset(results,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c028597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2049/2049 [08:13<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Detected Plates = 2006\n",
      "Number of Detected Plates = 2635\n",
      "Number of All Plates = 2164\n",
      "Recall = 0.9270\n",
      "Precision = 0.7613\n",
      "Characters in Detected Plates =  12006\n",
      "Error Characters (Detected) = 2102\n",
      "World Error Rate (Detected) = 0.1751\n",
      "\n",
      "Number of Perfectly Recognized Plates =  788\n",
      "Accuracy(Detected) = 0.3928\n",
      "Accuracy(Groundtruth) = 0.3641\n"
     ]
    }
   ],
   "source": [
    "results_aug = {}\n",
    "for f in tqdm(img_paths, ncols=80):\n",
    "    if f.endswith('.jpg'):\n",
    "        bname = os.path.splitext(f)[0]\n",
    "        img_p = img_dir + f\n",
    "        \n",
    "        ret, meta = run_aug(img_p,ratio=0.5)\n",
    "        \n",
    "        if ret[0] is not None:\n",
    "            output = affine_transform(ret, meta) # output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "            results_aug[bname] = output\n",
    "eval_dataset(results_aug,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "336ccc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2049/2049 [07:29<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Detected Plates = 1842\n",
      "Number of Detected Plates = 2461\n",
      "Number of All Plates = 2164\n",
      "Recall = 0.8512\n",
      "Precision = 0.7485\n",
      "Characters in Detected Plates =  11027\n",
      "Error Characters (Detected) = 2881\n",
      "World Error Rate (Detected) = 0.2613\n",
      "\n",
      "Number of Perfectly Recognized Plates =  493\n",
      "Accuracy(Detected) = 0.2676\n",
      "Accuracy(Groundtruth) = 0.2278\n"
     ]
    }
   ],
   "source": [
    "results_aug = {}\n",
    "for f in tqdm(img_paths, ncols=80):\n",
    "    if f.endswith('.jpg'):\n",
    "        bname = os.path.splitext(f)[0]\n",
    "        img_p = img_dir + f\n",
    "        \n",
    "        ret, meta = run_aug(img_p,ratio=0.8)\n",
    "        \n",
    "        if ret[0] is not None:\n",
    "            output = affine_transform(ret, meta) # output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "            results_aug[bname] = output\n",
    "eval_dataset(results_aug,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32b72db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2049/2049 [08:07<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Detected Plates = 1463\n",
      "Number of Detected Plates = 1960\n",
      "Number of All Plates = 2164\n",
      "Recall = 0.6761\n",
      "Precision = 0.7464\n",
      "Characters in Detected Plates =  8763\n",
      "Error Characters (Detected) = 3603\n",
      "World Error Rate (Detected) = 0.4112\n",
      "\n",
      "Number of Perfectly Recognized Plates =  194\n",
      "Accuracy(Detected) = 0.1326\n",
      "Accuracy(Groundtruth) = 0.0896\n"
     ]
    }
   ],
   "source": [
    "results_aug = {}\n",
    "for f in tqdm(img_paths, ncols=80):\n",
    "    if f.endswith('.jpg'):\n",
    "        bname = os.path.splitext(f)[0]\n",
    "        img_p = img_dir + f\n",
    "        \n",
    "        ret, meta = run_aug(img_p)\n",
    "        \n",
    "        if ret[0] is not None:\n",
    "            output = affine_transform(ret, meta) # output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "            results_aug[bname] = output\n",
    "eval_dataset(results_aug,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f53a23c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2049/2049 [25:44<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Detected Plates = 1954\n",
      "Number of Detected Plates = 2225\n",
      "Number of All Plates = 2164\n",
      "Recall = 0.9030\n",
      "Precision = 0.8782\n",
      "Characters in Detected Plates =  11691\n",
      "Error Characters (Detected) = 1666\n",
      "World Error Rate (Detected) = 0.1425\n",
      "\n",
      "Number of Perfectly Recognized Plates =  946\n",
      "Accuracy(Detected) = 0.4841\n",
      "Accuracy(Groundtruth) = 0.4372\n"
     ]
    }
   ],
   "source": [
    "results_diffusion = {}\n",
    "for f in tqdm(img_paths, ncols=80):\n",
    "    if f.endswith('.jpg'):\n",
    "        bname = os.path.splitext(f)[0]\n",
    "        img_p = img_dir + f\n",
    "        \n",
    "        ret, meta = run_withdiffusion(img_p)\n",
    "        \n",
    "        if ret[0] is not None:\n",
    "            output = affine_transform(ret, meta) # output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "            results_diffusion[bname] = output\n",
    "eval_dataset(results_diffusion,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f64f9daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2049/2049 [26:28<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Detected Plates = 1891\n",
      "Number of Detected Plates = 2149\n",
      "Number of All Plates = 2164\n",
      "Recall = 0.8738\n",
      "Precision = 0.8799\n",
      "Characters in Detected Plates =  11324\n",
      "Error Characters (Detected) = 1837\n",
      "World Error Rate (Detected) = 0.1622\n",
      "\n",
      "Number of Perfectly Recognized Plates =  841\n",
      "Accuracy(Detected) = 0.4447\n",
      "Accuracy(Groundtruth) = 0.3886\n"
     ]
    }
   ],
   "source": [
    "results_aug_diffusion = {}\n",
    "for f in tqdm(img_paths, ncols=80):\n",
    "    if f.endswith('.jpg'):\n",
    "        bname = os.path.splitext(f)[0]\n",
    "        img_p = img_dir + f\n",
    "        \n",
    "        ret, meta = run_aug_withdiffusion(img_p,ratio=0.1)\n",
    "        \n",
    "        if ret[0] is not None:\n",
    "            output = affine_transform(ret, meta) # output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "            results_aug_diffusion[bname] = output\n",
    "eval_dataset(results_aug_diffusion,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8acf93c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2049/2049 [27:21<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Detected Plates = 1824\n",
      "Number of Detected Plates = 2114\n",
      "Number of All Plates = 2164\n",
      "Recall = 0.8429\n",
      "Precision = 0.8628\n",
      "Characters in Detected Plates =  10924\n",
      "Error Characters (Detected) = 2271\n",
      "World Error Rate (Detected) = 0.2079\n",
      "\n",
      "Number of Perfectly Recognized Plates =  656\n",
      "Accuracy(Detected) = 0.3596\n",
      "Accuracy(Groundtruth) = 0.3031\n"
     ]
    }
   ],
   "source": [
    "results_aug_diffusion = {}\n",
    "for f in tqdm(img_paths, ncols=80):\n",
    "    if f.endswith('.jpg'):\n",
    "        bname = os.path.splitext(f)[0]\n",
    "        img_p = img_dir + f\n",
    "        \n",
    "        ret, meta = run_aug_withdiffusion(img_p,ratio=0.5)\n",
    "        \n",
    "        if ret[0] is not None:\n",
    "            output = affine_transform(ret, meta) # output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "            results_aug_diffusion[bname] = output\n",
    "eval_dataset(results_aug_diffusion,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3a33a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2049/2049 [26:57<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Detected Plates = 1411\n",
      "Number of Detected Plates = 1733\n",
      "Number of All Plates = 2164\n",
      "Recall = 0.6520\n",
      "Precision = 0.8142\n",
      "Characters in Detected Plates =  8460\n",
      "Error Characters (Detected) = 3217\n",
      "World Error Rate (Detected) = 0.3803\n",
      "\n",
      "Number of Perfectly Recognized Plates =  240\n",
      "Accuracy(Detected) = 0.1701\n",
      "Accuracy(Groundtruth) = 0.1109\n"
     ]
    }
   ],
   "source": [
    "results_aug_diffusion = {}\n",
    "for f in tqdm(img_paths, ncols=80):\n",
    "    if f.endswith('.jpg'):\n",
    "        bname = os.path.splitext(f)[0]\n",
    "        img_p = img_dir + f\n",
    "        \n",
    "        ret, meta = run_aug_withdiffusion(img_p,ratio=0.8)\n",
    "        \n",
    "        if ret[0] is not None:\n",
    "            output = affine_transform(ret, meta) # output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "            results_aug_diffusion[bname] = output\n",
    "eval_dataset(results_aug_diffusion,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c820838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2049/2049 [25:40<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Correctly Detected Plates = 794\n",
      "Number of Detected Plates = 1106\n",
      "Number of All Plates = 2164\n",
      "Recall = 0.3669\n",
      "Precision = 0.7179\n",
      "Characters in Detected Plates =  4762\n",
      "Error Characters (Detected) = 4252\n",
      "World Error Rate (Detected) = 0.8929\n",
      "\n",
      "Number of Perfectly Recognized Plates =  2\n",
      "Accuracy(Detected) = 0.0025\n",
      "Accuracy(Groundtruth) = 0.0009\n"
     ]
    }
   ],
   "source": [
    "results_aug_diffusion = {}\n",
    "for f in tqdm(img_paths, ncols=80):\n",
    "    if f.endswith('.jpg'):\n",
    "        bname = os.path.splitext(f)[0]\n",
    "        img_p = img_dir + f\n",
    "        \n",
    "        ret, meta = run_aug_withdiffusion(img_p)\n",
    "        \n",
    "        if ret[0] is not None:\n",
    "            output = affine_transform(ret, meta) # output = [num_objs, 6(x1, y1, x2, y2, conf, cls)]\n",
    "            results_aug_diffusion[bname] = output\n",
    "eval_dataset(results_aug_diffusion,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37bc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple Sequential model\n",
    "model2 = nn.Sequential(\n",
    "    nn.Linear(10, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")\n",
    "\n",
    "# Register hooks to get intermediate outputs\n",
    "outputs_test = []\n",
    "\n",
    "def _hook(module, input, output):\n",
    "    outputs_test.append(output)\n",
    "    \n",
    "    \n",
    "\n",
    "# Registering hooks for all children\n",
    "for layer in model2.children():\n",
    "    layer.register_forward_hook(_hook)\n",
    "\n",
    "# Sample input tensor\n",
    "input_tensor = torch.randn(1, 10).cuda().half()\n",
    "\n",
    "# Forward pass\n",
    "model2.cuda().half()\n",
    "model2(input_tensor)\n",
    "\n",
    "# Display outputs from all layers\n",
    "for i, output in enumerate(outputs_test, 1):\n",
    "    print(f\"Output from layer {i}: {output.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e0916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_AC_10.jpg\n",
      "torch.Size([1, 3, 1024, 1024])\n",
      "Output from layer 0: torch.Size([1, 32, 512, 512])\n",
      "Output from layer 1: torch.Size([1, 64, 256, 256])\n",
      "Output from layer 2: torch.Size([1, 32, 256, 256])\n",
      "Output from layer 3: torch.Size([1, 32, 256, 256])\n",
      "Output from layer 4: torch.Size([1, 32, 256, 256])\n",
      "Output from layer 5: torch.Size([1, 32, 256, 256])\n",
      "Output from layer 6: torch.Size([1, 128, 256, 256])\n",
      "Output from layer 7: torch.Size([1, 64, 256, 256])\n",
      "Output from layer 8: torch.Size([1, 64, 128, 128])\n",
      "Output from layer 9: torch.Size([1, 64, 128, 128])\n",
      "Output from layer 10: torch.Size([1, 64, 128, 128])\n",
      "Output from layer 11: torch.Size([1, 64, 128, 128])\n",
      "Output from layer 12: torch.Size([1, 64, 128, 128])\n",
      "Output from layer 13: torch.Size([1, 256, 128, 128])\n",
      "Output from layer 14: torch.Size([1, 128, 128, 128])\n",
      "Output from layer 15: torch.Size([1, 128, 64, 64])\n",
      "Output from layer 16: torch.Size([1, 128, 64, 64])\n",
      "Output from layer 17: torch.Size([1, 128, 64, 64])\n",
      "Output from layer 18: torch.Size([1, 128, 64, 64])\n",
      "Output from layer 19: torch.Size([1, 128, 64, 64])\n",
      "Output from layer 20: torch.Size([1, 512, 64, 64])\n",
      "Output from layer 21: torch.Size([1, 256, 64, 64])\n",
      "Output from layer 22: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 23: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 24: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 25: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 26: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 27: torch.Size([1, 1024, 32, 32])\n",
      "Output from layer 28: torch.Size([1, 512, 32, 32])\n",
      "Output from layer 29: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 30: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 31: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 32: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 33: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 34: torch.Size([1, 1024, 32, 32])\n",
      "Output from layer 35: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 36: torch.Size([1, 512, 32, 32])\n",
      "Output from layer 37: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 38: torch.Size([1, 128, 32, 32])\n",
      "Output from layer 39: torch.Size([1, 128, 64, 64])\n",
      "Output from layer 40: torch.Size([1, 128, 64, 64])\n",
      "Output from layer 41: torch.Size([1, 256, 64, 64])\n",
      "Output from layer 42: torch.Size([1, 64, 64, 64])\n",
      "Output from layer 43: torch.Size([1, 64, 64, 64])\n",
      "Output from layer 44: torch.Size([1, 64, 64, 64])\n",
      "Output from layer 45: torch.Size([1, 64, 64, 64])\n",
      "Output from layer 46: torch.Size([1, 256, 64, 64])\n",
      "Output from layer 47: torch.Size([1, 128, 64, 64])\n",
      "Output from layer 48: torch.Size([1, 64, 64, 64])\n",
      "Output from layer 49: torch.Size([1, 64, 128, 128])\n",
      "Output from layer 50: torch.Size([1, 64, 128, 128])\n",
      "Output from layer 51: torch.Size([1, 128, 128, 128])\n",
      "Output from layer 52: torch.Size([1, 32, 128, 128])\n",
      "Output from layer 53: torch.Size([1, 32, 128, 128])\n",
      "Output from layer 54: torch.Size([1, 32, 128, 128])\n",
      "Output from layer 55: torch.Size([1, 32, 128, 128])\n",
      "Output from layer 56: torch.Size([1, 128, 128, 128])\n",
      "Output from layer 57: torch.Size([1, 64, 128, 128])\n",
      "Output from layer 58: torch.Size([1, 128, 64, 64])\n",
      "Output from layer 59: torch.Size([1, 256, 64, 64])\n",
      "Output from layer 60: torch.Size([1, 64, 64, 64])\n",
      "Output from layer 61: torch.Size([1, 64, 64, 64])\n",
      "Output from layer 62: torch.Size([1, 64, 64, 64])\n",
      "Output from layer 63: torch.Size([1, 64, 64, 64])\n",
      "Output from layer 64: torch.Size([1, 256, 64, 64])\n",
      "Output from layer 65: torch.Size([1, 128, 64, 64])\n",
      "Output from layer 66: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 67: torch.Size([1, 512, 32, 32])\n",
      "Output from layer 68: torch.Size([1, 128, 32, 32])\n",
      "Output from layer 69: torch.Size([1, 128, 32, 32])\n",
      "Output from layer 70: torch.Size([1, 128, 32, 32])\n",
      "Output from layer 71: torch.Size([1, 128, 32, 32])\n",
      "Output from layer 72: torch.Size([1, 512, 32, 32])\n",
      "Output from layer 73: torch.Size([1, 256, 32, 32])\n",
      "Output from layer 74: torch.Size([1, 128, 128, 128])\n",
      "Output from layer 75: torch.Size([1, 256, 64, 64])\n",
      "Output from layer 76: torch.Size([1, 512, 32, 32])\n",
      "77\n",
      "Output from layer 77: torch.Size([1, 64512, 40])\n",
      "77\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31383/3299742239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Output from layer {i}: {output.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31383/3299742239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayers_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Output from layer {i}: {layers_.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "layers_outputs = []\n",
    "def _hook(module, input, output):\n",
    "    layers_outputs.append(output)\n",
    "\n",
    "    \n",
    "print(img_paths[1])\n",
    "img, meta = preproccess_img(img_dir+img_paths[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for layer in model.model.children():\n",
    "    layer.register_forward_hook(_hook)\n",
    "\n",
    "\n",
    "x = img\n",
    "dsa = model(x)\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(dsa.shape)\n",
    "#print(layers_outputs)\n",
    "for i, output in enumerate(layers_outputs, 0):\n",
    "    try:\n",
    "        print(f\"Output from layer {i}: {output.size()}\")\n",
    "    except:\n",
    "        for layers_ in output:\n",
    "            print(i)\n",
    "            print(f\"Output from layer {i}: {layers_.size()}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22118955",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_plate = {}\n",
    "total_p, correct_p = 0, 0\n",
    "for k, v in tqdm(label_data.items(), ncols=80):\n",
    "    gt = transfer_label(v)\n",
    "    total_p += len(gt)\n",
    "    if k in results:\n",
    "        r = group_plate(results[k]) # r = [{'plate': int[x1, y1, x2, y2], 'char':[int[x1, y1, x2, y2, label], ...]}, ...]\n",
    "        correct_r = []\n",
    "        for _r in r:\n",
    "            for idx, bbox in enumerate(gt):\n",
    "                if get_iou(bbox, _r['plate']) >= 0.5:\n",
    "                    n_r = {'plate': _r['plate'], 'char': _r['char'], 'idx': idx}\n",
    "                    correct_r.append(n_r)\n",
    "        num = len(correct_r)\n",
    "        if num > 0:\n",
    "            correct_plate[k] = correct_r\n",
    "            correct_p += num\n",
    "            \n",
    "print(\"Number of Correctly Detected Plates =\", correct_p)\n",
    "print(\"Number of All Plates =\", total_p)\n",
    "print(\"Accuracy (Detected Plates) = {:.4f}\".format(correct_p/total_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a69583",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_perfect = 0  ### number of perfectly recognized plates\n",
    "n_sid = 0  ### number of failed recognized chars in detected\n",
    "n_detected = 0  ### number of chars in detected plates\n",
    "\n",
    "for k, v in tqdm(label_data.items(), ncols=80):\n",
    "    gt_strs = [s.split(',')[-1] for s in v]\n",
    "    if k in correct_plate:\n",
    "        objs = correct_plate[k]\n",
    "        for obj in objs: # obj = [{'plate': int[x1, y1, x2, y2], 'char':[int[x1, y1, x2, y2, label], ...], 'idx': int(i)}]\n",
    "            pred_str = [classes[x[-1]] for x in obj['char']]\n",
    "            wer, sid, t = get_wer(list(gt_strs[obj['idx']]), pred_str)\n",
    "            n_sid += sid\n",
    "            n_detected += t\n",
    "            if wer == 0:\n",
    "                n_perfect += 1\n",
    "\n",
    "\n",
    "print(\"Characters in Detected Plates = \", n_detected)\n",
    "print(\"Error Characters (Detected) =\", n_sid)\n",
    "print(\"World Error Rate (Detected) = {:.4f}\".format(n_sid/n_detected))\n",
    "\n",
    "print(\"\\nNumber of Perfectly Recognized Plates = \", n_perfect)\n",
    "print(\"Accuracy(Detected) = {:.4f}\".format(n_perfect/correct_p))\n",
    "print(\"Accuracy(Groundtruth) = {:.4f}\".format(n_perfect/total_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_plate = {}\n",
    "total_p, correct_p = 0, 0\n",
    "for k, v in tqdm(label_data.items(), ncols=80):\n",
    "    gt = transfer_label(v)\n",
    "    total_p += len(gt)\n",
    "    if k in results_aug:\n",
    "        r = group_plate(results_aug[k]) # r = [{'plate': int[x1, y1, x2, y2], 'char':[int[x1, y1, x2, y2, label], ...]}, ...]\n",
    "        correct_r = []\n",
    "        for _r in r:\n",
    "            for idx, bbox in enumerate(gt):\n",
    "                if get_iou(bbox, _r['plate']) >= 0.5:\n",
    "                    n_r = {'plate': _r['plate'], 'char': _r['char'], 'idx': idx}\n",
    "                    correct_r.append(n_r)\n",
    "        num = len(correct_r)\n",
    "        if num > 0:\n",
    "            correct_plate[k] = correct_r\n",
    "            correct_p += num\n",
    "            \n",
    "print(\"Number of Correctly Detected Plates =\", correct_p)\n",
    "print(\"Number of All Plates =\", total_p)\n",
    "print(\"Accuracy (Detected Plates) = {:.4f}\".format(correct_p/total_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_perfect = 0  ### number of perfectly recognized plates\n",
    "n_sid = 0  ### number of failed recognized chars in detected\n",
    "n_detected = 0  ### number of chars in detected plates\n",
    "\n",
    "for k, v in tqdm(label_data.items(), ncols=80):\n",
    "    gt_strs = [s.split(',')[-1] for s in v]\n",
    "    if k in correct_plate:\n",
    "        objs = correct_plate[k]\n",
    "        for obj in objs: # obj = [{'plate': int[x1, y1, x2, y2], 'char':[int[x1, y1, x2, y2, label], ...], 'idx': int(i)}]\n",
    "            pred_str = [classes[x[-1]] for x in obj['char']]\n",
    "            wer, sid, t = get_wer(list(gt_strs[obj['idx']]), pred_str)\n",
    "            n_sid += sid\n",
    "            n_detected += t\n",
    "            if wer == 0:\n",
    "                n_perfect += 1\n",
    "\n",
    "\n",
    "print(\"Characters in Detected Plates = \", n_detected)\n",
    "print(\"Error Characters (Detected) =\", n_sid)\n",
    "print(\"World Error Rate (Detected) = {:.4f}\".format(n_sid/n_detected))\n",
    "\n",
    "print(\"\\nNumber of Perfectly Recognized Plates = \", n_perfect)\n",
    "print(\"Accuracy(Detected) = {:.4f}\".format(n_perfect/correct_p))\n",
    "print(\"Accuracy(Groundtruth) = {:.4f}\".format(n_perfect/total_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8d91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
